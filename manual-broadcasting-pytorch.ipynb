{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN04Qr4xjpNEKgsstJ26Gu4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# part A - .expand_as():"],"metadata":{"id":"cQ6U59oMWbe0"}},{"cell_type":"markdown","source":["In this section, we implemented the my_expand_as function, which manually simulates PyTorch’s expand_as functionality without using built-in broadcasting functions.\n","\n","The function:\n","*   Adds leading dimensions of size 1 to match the shape of the target tensor B\n","*   Iterates through each dimension and checks if broadcasting is possible\n","*   If a dimension in A is 1 and needs to match a larger size in B, it duplicates the data using select and stack\n","*   If broadcasting is not possible (i.e., incompatible shapes), it raises a ValueError"],"metadata":{"id":"LioA2mEvWW4c"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FctRPmbi94U8"},"outputs":[],"source":["import torch\n","\n","def my_expand_as(A, B):\n","    shape_a = list(A.shape) # Dimensions of A.\n","    shape_b = list(B.shape) # Dimensions of B.\n","\n","    # Adding dimensions to A according to B.\n","    while len(shape_a) < len(shape_b):\n","        A = A.unsqueeze(0)\n","        shape_a = list(A.shape)\n","\n","    # Checking whether it is possible to broadcast.\n","    output = A.clone() # Duplication the Tensor.\n","    for i in range(len(shape_b)):\n","        dim_a = output.shape[i]\n","        dim_b = shape_b[i]\n","\n","        if dim_a == dim_b: # The sizes of the dimensions are equal.\n","            continue  #  There is no need to broadcast.\n","        elif dim_a == 1: # The sizes of the dimensions are not equal, But the size of dim A is 1.\n","            slices = [output.select(i, 0) for _ in range(dim_b)] # Create copies of the single element in dimension i.\n","            output = torch.stack(slices, dim=i) # Rebuild tensor with expanded dimension i using the copies\n","\n","        else:\n","            raise ValueError(f\"Cannot broadcast dimension {dim_a} to {dim_b}\")\n","\n","    return output"]},{"cell_type":"code","source":["# Basic function validation check\n","def check_my_func(C, C_2):\n","    if torch.equal(C, C_2):\n","        print(\"True\")\n","    else:\n","        print(\"False\")\n","\n","# --- Test 1 ---\n","A = torch.tensor([[1], [2]])        # shape: [2,1]\n","B = torch.zeros(2, 3)               # shape: [2,3]\n","C = my_expand_as(A, B)\n","C_2 = A.expand_as(B)\n","check_my_func(C, C_2)\n","\n","# --- Test 2 ---\n","A = torch.tensor([[[1]]])           # shape: [1,1,1]\n","B = torch.zeros(2, 3, 4)            # shape: [2,3,4]\n","C = my_expand_as(A, B)\n","C_2 = A.expand_as(B)\n","check_my_func(C, C_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cK-hvmq6WLTJ","executionInfo":{"status":"ok","timestamp":1742659331239,"user_tz":-120,"elapsed":10,"user":{"displayName":"ido amar","userId":"01133300011231872560"}},"outputId":"162563cb-9022-4e9d-9abd-e625da8a6560"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","True\n"]}]},{"cell_type":"markdown","source":["# Part B - is_broadcastable:"],"metadata":{"id":"aCeGLMFAYL9j"}},{"cell_type":"markdown","source":["In this section, we implement the is_broadcastable function, which checks whether two tensors A and B can be broadcast together according to PyTorch's broadcasting rules.\n","\n","Instead of modifying the tensors themselves, we only work with their shapes (A.shape and B.shape).\n","We align the shapes by prepending dimensions of size 1 when necessary, and check if each dimension pair is either equal or one of them is 1.\n","\n","The function returns:\n","*   True and the resulting broadcasted shape (as a tuple) if broadcasting is possible.\n","*   False and None if the shapes are not compatible for broadcasting."],"metadata":{"id":"y6t1Hu3iaSFS"}},{"cell_type":"code","source":["def is_broadcastable(A, B):\n","    shape_a = list(A.shape)  # Get shape of A.\n","    shape_b = list(B.shape)  # Get shape of B.\n","\n","    # Add 1 as need to the shorter shape so both shapes have the same length.\n","    while len(shape_a) < len(shape_b):\n","        shape_a = [1] + shape_a\n","    while len(shape_b) < len(shape_a):\n","        shape_b = [1] + shape_b\n","\n","    result_shape = []  # Will hold the final broadcasted shape.\n","\n","    # Check each dimension if its broadcastable.\n","    for dim_a, dim_b in zip(shape_a, shape_b):\n","        if dim_a == dim_b:\n","            result_shape.append(dim_a)  # Same size.\n","        elif dim_a == 1:\n","            result_shape.append(dim_b)  # A can be broadcast to B.\n","        elif dim_b == 1:\n","            result_shape.append(dim_a)  # B can be broadcast to A.\n","        else:\n","            return False, None  # Cannot broadcast\n","\n","    return True, tuple(result_shape)  # Return success and the broadcasted shape"],"metadata":{"id":"ZVWsctegYF14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Basic function validation check\n","A = torch.tensor([[1], [2]])         # shape: [2, 1]\n","B = torch.tensor([[10, 20, 30]])     # shape: [1, 3]\n","print(is_broadcastable(A, B))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4HUE5tYXaSu9","executionInfo":{"status":"ok","timestamp":1742659332173,"user_tz":-120,"elapsed":20,"user":{"displayName":"ido amar","userId":"01133300011231872560"}},"outputId":"d4716b16-2a65-4483-fb69-1d660b60f6ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(True, (2, 3))\n"]}]},{"cell_type":"markdown","source":["# Part C - .torch.broadcast_tensors:"],"metadata":{"id":"8WtBIbT6fYvj"}},{"cell_type":"markdown","source":["This function performs addition between two tensors.\n","It uses the functions is_broadcastable and my_expand_as to reshape both tensors manually before performing the addition.\n"],"metadata":{"id":"YTRTn_IKfcxz"}},{"cell_type":"code","source":["def my_broadcast(A, B):\n","    # Check if tensors can be broadcasted.\n","    can_broadcast, target_shape = is_broadcastable(A, B)\n","    if not can_broadcast: # If can't broadcast.\n","        raise ValueError(\"Tensors cannot be broadcasted\")\n","\n","    # Expand A and B to the target shape.\n","    A_exp = my_expand_as(A, torch.empty(target_shape))\n","    B_exp = my_expand_as(B, torch.empty(target_shape))\n","\n","    # Return both expanded tensors\n","    return A_exp, B_exp"],"metadata":{"id":"BaZgZXwAfYbH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part D - test:"],"metadata":{"id":"q1AeIkjIhQ0d"}},{"cell_type":"markdown","source":["In this section, we test our custom implementation of broadcast_tensors (my_broadcast)\n","by comparing it with PyTorch's built-in function torch.broadcast_tensors.\n","\n","Each test checks whether the two input tensors are broadcast correctly,\n","and whether the output tensors match PyTorch's results."],"metadata":{"id":"C_1ueq_ThUEZ"}},{"cell_type":"code","source":["# Function to compare my manual broadcasting implementation with PyTorch's broadcast_tensors\n","def check_broadcast_tensors(A, B):\n","    try:\n","        # Try to apply PyTorch's built-in broadcasting\n","        A_expected, B_expected = torch.broadcast_tensors(A, B)\n","        expected_success = True  # Mark that broadcasting succeeded\n","    except RuntimeError as e:\n","        # PyTorch failed to broadcast the tensors\n","        expected_success = False\n","        expected_error = str(e)  # Save the error message for comparison\n","\n","    try:\n","        # Try to apply my manual broadcasting implementation\n","        A_result, B_result = my_broadcast(A, B)\n","        result_success = True  # Mark that broadcasting succeeded\n","    except Exception as e:\n","        # My function failed to broadcast the tensors\n","        result_success = False\n","        result_error = str(e)  # Save the error message for comparison\n","\n","    # Compare the results of both implementations\n","    if expected_success and result_success:\n","        # Both succeeded – now compare the actual broadcasted tensors\n","        same_A = torch.equal(A_expected, A_result)\n","        same_B = torch.equal(B_expected, B_result)\n","        print(f\"✅ Both succeeded.\\n\\tA match: {same_A} \\n\\tB match: {same_B}\")\n","    elif not expected_success and not result_success:\n","        # Both implementations raised an error – that's correct\n","        print(\"✅ Both raised an error as expected\")\n","    else:\n","        # One succeeded while the other failed – mismatch\n","        print(\"❌ The results are not the same\")\n","        if expected_success:\n","            print(f\"Expected success, got error: {result_error}\")\n","        else:\n","            print(f\"Expected error: {expected_error}, but function returned a result.\")\n"],"metadata":{"id":"u36g1mLzhpe6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test cases\n","\n","print(\"Test 1: [2,1] and [1,3]\")\n","check_broadcast_tensors(torch.tensor([[1], [2]]), torch.tensor([[10, 20, 30]]))\n","\n","print(\"\\nTest 2: [3] and [2,1]\")\n","check_broadcast_tensors(torch.tensor([1, 2, 3]), torch.tensor([[1], [2]]))\n","\n","print(\"\\nTest 3: [1,1,1] and [2,3,4]\")\n","check_broadcast_tensors(torch.tensor([[[1]]]), torch.empty(2, 3, 4))\n","\n","print(\"\\nTest 4: Incompatible shapes [2] and [3]\")\n","check_broadcast_tensors(torch.tensor([2, 3]), torch.tensor([3, 4]))\n","\n","print(\"\\nTest 5: [1,3] and [3]\")\n","check_broadcast_tensors(torch.tensor([[1, 2, 3]]), torch.tensor([10, 20, 30]))\n","\n","print(\"\\nTest 6: Incompatible shapes [2] and [3]\")\n","check_broadcast_tensors(torch.tensor([1, 2]), torch.tensor([1, 2, 3]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IqimXWXShyFI","executionInfo":{"status":"ok","timestamp":1742659333664,"user_tz":-120,"elapsed":12,"user":{"displayName":"ido amar","userId":"01133300011231872560"}},"outputId":"3e555e06-7011-4887-b84f-709a91842852"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test 1: [2,1] and [1,3]\n","✅ Both succeeded.\n","\tA match: True \n","\tB match: True\n","\n","Test 2: [3] and [2,1]\n","✅ Both succeeded.\n","\tA match: True \n","\tB match: True\n","\n","Test 3: [1,1,1] and [2,3,4]\n","✅ Both succeeded.\n","\tA match: True \n","\tB match: True\n","\n","Test 4: Incompatible shapes [2] and [3]\n","✅ Both succeeded.\n","\tA match: True \n","\tB match: True\n","\n","Test 5: [1,3] and [3]\n","✅ Both succeeded.\n","\tA match: True \n","\tB match: True\n","\n","Test 6: Incompatible shapes [2] and [3]\n","✅ Both raised an error as expected\n"]}]},{"cell_type":"markdown","source":["# Summary:"],"metadata":{"id":"tkZ041JAmf77"}},{"cell_type":"markdown","source":["In this assignment, we manually implemented key broadcasting operations in PyTorch without using any built-in broadcasting functions such as expand, broadcast_to, or broadcast_tensors.\n","\n","The main goals of the assignment were:\n","\n","*   Implement my_expand_as(A, B) to manually expand a tensor to match a target shape\n","*   Implement is_broadcastable(A, B) to check whether two tensors can be broadcast together and return the resulting shape\n","*   Implement my_broadcast(A, B) to simulate torch.broadcast_tensors, returning both broadcast tensors\n","*   Compare the custom implementations with PyTorch's behavior through a set of test cases, including both valid and invalid scenarios\n","\n","This exercise deepened our understanding of how broadcasting works under the hood and how to manually manipulate tensor shapes using operations like `unsqueeze`, `stack`, and `select`."],"metadata":{"id":"5aVpjEuWmj3M"}}]}